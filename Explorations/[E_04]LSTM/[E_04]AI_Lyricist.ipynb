{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "[E_04]AI_Lyricist.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9eDbsmuJNK6z"
      },
      "source": [
        "# 프로젝트 : 멋진 작사가 만들기 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZcD8LtcCNRNm"
      },
      "source": [
        "## STEP1. 데이터 다운로드\n",
        "`~/aiffel/lyricist/data/lyrics`에 데이터 넣어 놓음"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7gCJPSu2NdSg"
      },
      "source": [
        "## STEP2. 데이터 읽어오기 \n",
        "`glob` 모듈을 사용하면 파일을 읽어오는 작업을 하기가 아주 용이\n",
        "\n",
        "`glob` 를 활용하여 모든 `txt` 파일을 읽어온 후, `raw_corpus` 리스트에 문장 단위로 저장"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y60y1pdsNnEz",
        "outputId": "a38f7ea9-cecb-4b1e-c28c-337a3900e43f"
      },
      "source": [
        "import glob, re, os\n",
        "\n",
        "txt_file_path = '/content/drive/MyDrive/aiffel/lyricist/data/lyrics/*'\n",
        "\n",
        "txt_list = glob.glob(txt_file_path)\n",
        "\n",
        "raw_corpus = []\n",
        "\n",
        "# 여러개의 txt 파일을 모두 읽어서 raw_corpus 에 담습니다.\n",
        "for txt_file in txt_list:\n",
        "    with open(txt_file, \"r\") as f:\n",
        "        raw = f.read().splitlines()\n",
        "        raw_corpus.extend(raw)\n",
        "\n",
        "print(\"데이터 크기:\", len(raw_corpus))\n",
        "print(\"Examples:\\n\", raw_corpus[:3])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "데이터 크기: 187088\n",
            "Examples:\n",
            " ['Looking for some education', 'Made my way into the night', 'All that bullshit conversation']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ChxeG4gsNoW0"
      },
      "source": [
        "## Step 3. 데이터 정제\n",
        "앞서 배운 테크닉들을 활용해 문장 생성에 적합한 모양새로 데이터를 정제하기\n",
        "\n",
        "`preprocess_sentence()` 함수를 활용해 데이터를 정제\n",
        "\n",
        "추가로 지나치게 긴 문장은 다른 데이터들이 과도한 Padding을 갖게 하므로 제거\n",
        "\n",
        "문장을 토큰화 했을 때 토큰의 개수가 15개를 넘어가는 문장을 학습 데이터에서 제외하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TU5PqQYkyEgT",
        "outputId": "4a813a4c-29b3-433b-cc45-091334a20423"
      },
      "source": [
        "def preprocess_sentence(sentence):\n",
        "    sentence = sentence.lower().strip() # 1\n",
        "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence) # 2\n",
        "    sentence = re.sub(r'[\" \"]+', \" \", sentence) # 3\n",
        "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence) # 4\n",
        "    sentence = sentence.strip() # 5\n",
        "    sentence = '<start> ' + sentence + ' <end>' # 6\n",
        "    return sentence\n",
        "\n",
        "\n",
        "print(preprocess_sentence(\"This @_is ;;;sample        sentence.\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<start> this is sample sentence . <end>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WtltpI68gWR3"
      },
      "source": [
        "문장 길이 0인것 제외, 문장 끝이 `:` 로 끝나는 문장을 제외하진 않음 (가사여서)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IQ0y33hQgNUh",
        "outputId": "370b76f5-f50b-4931-9663-82af8bb6ea65"
      },
      "source": [
        "corpus = []\n",
        "for sentence in raw_corpus:\n",
        "  if len(sentence) == 0: continue\n",
        "  if len(sentence.split(' ')) > 15: continue\n",
        "  corpus.append(preprocess_sentence(sentence))\n",
        "\n",
        "print(len(corpus))\n",
        "corpus[:10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "168535\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<start> looking for some education <end>',\n",
              " '<start> made my way into the night <end>',\n",
              " '<start> all that bullshit conversation <end>',\n",
              " '<start> baby , can t you read the signs ? i won t bore you with the details , baby <end>',\n",
              " '<start> i don t even wanna waste your time <end>',\n",
              " '<start> let s just say that maybe <end>',\n",
              " '<start> you could help me ease my mind <end>',\n",
              " '<start> i ain t mr . right but if you re looking for fast love <end>',\n",
              " '<start> if that s love in your eyes <end>',\n",
              " '<start> it s more than enough <end>']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-vWWZqeUgMl2"
      },
      "source": [
        "> 데이터의 수가 많다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MNTjL7S-1r5l",
        "outputId": "14ca2811-2e4d-4d36-f891-79deff4e809d"
      },
      "source": [
        "corpus = []\n",
        "for sentence in raw_corpus:\n",
        "  if len(sentence) == 0: continue\n",
        "  corpus.append(preprocess_sentence(sentence))\n",
        "print(len(corpus))\n",
        "corpus[:10]\n",
        "\n",
        "len_corpus = []\n",
        "for sentence in corpus:\n",
        "  if len(sentence.split()) <= 15:\n",
        "    len_corpus.append(sentence)\n",
        "  \n",
        "print(len(len_corpus))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "175986\n",
            "156227\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hxFnwX6tpPQx"
      },
      "source": [
        "여기서 토큰이 15개 이상인 문장을 날리고 싶어 `if len(sentence.split(' ')) > 15: continue` 을 사용했으나 생각보다 데이터의 수가 많아서 1차로 `corpus`에 길이가 0인것만 제외하고 `preprocess_sentence`를 해주고 2차로 `len_corpus`로 토큰의 길이가 15이하인 것만 가져왔다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rls4IeF0l04L"
      },
      "source": [
        "li1 = set(corpus)\n",
        "li2 = set(len_corpus)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2vu8-Q-xm6i-",
        "outputId": "03c1fe94-0066-4cb6-bedd-71c8df9bf44a"
      },
      "source": [
        "x = next(iter(li1-li2))\n",
        "print(len(x.split()))\n",
        "print(x)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "16\n",
            "<start> how many times can i say the same thing different ways that rhyme ? <end>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83CL7PhbN-He"
      },
      "source": [
        "##Step 4. 평가 데이터셋 분리\n",
        "훈련 데이터와 평가 데이터를 분리\n",
        "\n",
        "`tokenize()` 함수로 데이터를 Tensor로 변환한 후, `sklearn` 모듈의 `train_test_split()` 함수를 사용해 훈련 데이터와 평가 데이터를 분리 \n",
        "\n",
        "단어장의 크기는 12,000 이상 으로 설정하고 총 데이터의 20% 를 평가 데이터셋으로 사용"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T4x1O2R0j6o9"
      },
      "source": [
        "### Tensor 변환 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OQC1LQHA3P28",
        "outputId": "7dc8ed30-f55e-47a8-c050-99a5d18c1cd2"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "def tokenize(len_corpus):\n",
        "  tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
        "      num_words = 12000,\n",
        "      filters = ' ',\n",
        "      oov_token = \"<unk>\"\n",
        "  )\n",
        "  tokenizer.fit_on_texts(len_corpus)\n",
        "  tensor = tokenizer.texts_to_sequences(len_corpus)\n",
        "  tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding = 'post')\n",
        "\n",
        "  print(tensor, tokenizer)\n",
        "  return tensor, tokenizer\n",
        "\n",
        "tensor, tokenizer = tokenize(len_corpus)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[  2 291  28 ...   0   0   0]\n",
            " [  2 219  13 ...   0   0   0]\n",
            " [  2  25  15 ...   0   0   0]\n",
            " ...\n",
            " [  2  21  77 ...   0   0   0]\n",
            " [  2  42  26 ...   0   0   0]\n",
            " [  2  21  77 ...   0   0   0]] <keras_preprocessing.text.Tokenizer object at 0x7fbb1d22cc10>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1qH0NLUR5k7l",
        "outputId": "60ac7a05-ed37-41e3-c303-92422d603978"
      },
      "source": [
        "print(tensor[:3, :10])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[   2  291   28   94 4490    3    0    0    0    0]\n",
            " [   2  219   13   86  220    6  113    3    0    0]\n",
            " [   2   25   15 1039 2250    3    0    0    0    0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eKlCjDNZ5nYA",
        "outputId": "4d52f092-e382-4754-f1c3-cd2ba6e24cd7"
      },
      "source": [
        "for idx in tokenizer.index_word:\n",
        "    print(idx, \":\", tokenizer.index_word[idx])\n",
        "\n",
        "    if idx >= 10: break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1 : <unk>\n",
            "2 : <start>\n",
            "3 : <end>\n",
            "4 : i\n",
            "5 : ,\n",
            "6 : the\n",
            "7 : you\n",
            "8 : and\n",
            "9 : a\n",
            "10 : to\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "odKXi5DqOmww",
        "outputId": "dacfcc52-113c-4ebd-e052-4c8ca1ce94ca"
      },
      "source": [
        "# tensor에서 마지막 토큰을 잘라내서 소스 문장을 생성합니다\n",
        "# 마지막 토큰은 <end>가 아니라 <pad>일 가능성이 높습니다.\n",
        "src_input = tensor[:, :-1]  \n",
        "# tensor에서 <start>를 잘라내서 타겟 문장을 생성합니다.\n",
        "tgt_input = tensor[:, 1:]    \n",
        "\n",
        "print(src_input[0])\n",
        "print(tgt_input[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[   2  291   28   94 4490    3    0    0    0    0    0    0    0    0]\n",
            "[ 291   28   94 4490    3    0    0    0    0    0    0    0    0    0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SnGlwVO5kAIc"
      },
      "source": [
        "### 데이터셋 분리 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A8-ljn7DOHHE",
        "outputId": "925dec15-7fcb-453b-ee41-3b6974c335c7"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "enc_train, enc_val, dec_train, dec_val = train_test_split(src_input, tgt_input, test_size = 0.2, random_state = 42)\n",
        "\n",
        "print(\"Source Train:\", enc_train.shape)\n",
        "print(\"Target Train:\", dec_train.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Source Train: (124981, 14)\n",
            "Target Train: (124981, 14)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MogGTKsRS4yB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1fa41661-8886-45c2-dd57-ff2d63966db9"
      },
      "source": [
        "BUFFER_SIZE = len(enc_train)\n",
        "BATCH_SIZE = 256\n",
        "steps_per_epoch = len(enc_train) // BATCH_SIZE\n",
        "\n",
        " # tokenizer가 구축한 단어사전 내 7000개와, 여기 포함되지 않은 0:<pad>를 포함하여 7001개\n",
        "VOCAB_SIZE = tokenizer.num_words + 1   \n",
        "\n",
        "# 준비한 데이터 소스로부터 데이터셋을 만듭니다\n",
        "# 데이터셋에 대해서는 아래 문서를 참고하세요\n",
        "# 자세히 알아둘수록 도움이 많이 되는 중요한 문서입니다\n",
        "# https://www.tensorflow.org/api_docs/python/tf/data/Dataset\n",
        "dataset = tf.data.Dataset.from_tensor_slices((enc_train, dec_train))\n",
        "dataset = dataset.shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "dataset"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BatchDataset shapes: ((256, 14), (256, 14)), types: (tf.int32, tf.int32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eTBjYjTOlWCF",
        "outputId": "a592a56a-7271-4dd2-84c8-ba2149cc12f7"
      },
      "source": [
        "BUFFER_SIZE = len(enc_val)\n",
        "BATCH_SIZE = 256\n",
        "steps_per_epoch = len(enc_val) // BATCH_SIZE\n",
        "\n",
        " # tokenizer가 구축한 단어사전 내 7000개와, 여기 포함되지 않은 0:<pad>를 포함하여 7001개\n",
        "VOCAB_SIZE = tokenizer.num_words + 1   \n",
        "\n",
        "# 준비한 데이터 소스로부터 데이터셋을 만듭니다\n",
        "# 데이터셋에 대해서는 아래 문서를 참고하세요\n",
        "# 자세히 알아둘수록 도움이 많이 되는 중요한 문서입니다\n",
        "# https://www.tensorflow.org/api_docs/python/tf/data/Dataset\n",
        "valset = tf.data.Dataset.from_tensor_slices((enc_val, dec_val))\n",
        "valset = valset.shuffle(BUFFER_SIZE)\n",
        "valset = valset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "valset"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BatchDataset shapes: ((256, 14), (256, 14)), types: (tf.int32, tf.int32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UkcV1AIrOIBM"
      },
      "source": [
        "여기까지 올바르게 진행했을 경우, 아래 실행 결과를 확인할 수 있음"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U7z4BgjDOKbS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78519f53-8704-407e-aec8-53414e2de51f"
      },
      "source": [
        "print(\"Source Train:\", enc_train.shape)\n",
        "print(\"Target Train:\", dec_train.shape)\n",
        "# out:\n",
        "\n",
        "# Source Train: (124960, 14)\n",
        "# Target Train: (124960, 14)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Source Train: (124981, 14)\n",
            "Target Train: (124981, 14)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lRi690k8OMv5"
      },
      "source": [
        "## Step 5. 인공지능 만들기\n",
        "모델의 Embedding Size와 Hidden Size를 조절하며 10 Epoch 안에 val_loss 값을 2.2 수준으로 줄일 수 있는 모델을 설계하기 (Loss는 아래 제시된 Loss 함수를 그대로 사용!)\n",
        "\n",
        "모델이 생성한 가사 한 줄을 제출하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l6fCLkzzv_N9"
      },
      "source": [
        "class TextGenerator(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
        "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
        "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
        "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
        "        \n",
        "    def call(self, x):\n",
        "        out = self.embedding(x)\n",
        "        out = self.rnn_1(out)\n",
        "        out = self.rnn_2(out)\n",
        "        out = self.linear(out)\n",
        "        \n",
        "        return out\n",
        "    \n",
        "embedding_size = 256\n",
        "hidden_size = 1024\n",
        "model = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oYrt_TZdP3BE"
      },
      "source": [
        "* batch size = 모델이 경사하강법을 통해 손실/오차를 계산해서 모델의 가중치를 업데이트할 때 한번에 몇 개의 관측치를 사용하는지를 결정하는 파라미터 (대체적으로 32~512 사이의 2의 제곱수 사용)\n",
        "\n",
        "  가중치를 업데이트 할 수 있을 만큼의 충분한 정보를 제공할 수 있는 충분한 양의 관측치의 크기를 확인하기 위한 변수\n",
        "\n",
        "  * 너무 큰 배치를 고를 경우 \n",
        "    1. 모든 데이터에 대한 Loss를 계산해야 하는 문제점\n",
        "    2. 주어진 epoch 안에 가중치를 충분히 업데이트 할 만큼의 iteration을 돌릴 수 없음(학습 효과 저하)\n",
        "\n",
        "  * 너무 작은 배치 크기를 고르는 경우\n",
        "    1. 학습에 오랜 시간\n",
        "    2. 추정값에 노이즈 증가 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x2NyB2gznbAW",
        "outputId": "94b5a406-9e54-430e-8815-db244f10fb6b"
      },
      "source": [
        "# 데이터셋에서 데이터 한 배치만 불러오는 방법입니다.\n",
        "# 지금은 동작 원리에 너무 빠져들지 마세요~\n",
        "for src_sample, tgt_sample in dataset.take(1): break\n",
        "\n",
        "# 한 배치만 불러온 데이터를 모델에 넣어봅니다\n",
        "model(src_sample)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(256, 14, 12001), dtype=float32, numpy=\n",
              "array([[[ 2.01889692e-04,  4.53735774e-05,  1.70246276e-04, ...,\n",
              "         -6.91563546e-05,  6.59728612e-05,  6.05093519e-05],\n",
              "        [ 3.69252230e-04,  1.34047063e-04,  2.42031660e-04, ...,\n",
              "         -1.21333069e-04,  4.97304136e-04, -8.28525299e-05],\n",
              "        [ 4.76705842e-04,  2.84860056e-04,  5.82214197e-05, ...,\n",
              "          7.65052609e-05,  7.96879933e-04,  1.02770891e-05],\n",
              "        ...,\n",
              "        [ 1.86874051e-04, -1.90771912e-04,  6.61871862e-04, ...,\n",
              "         -1.95370824e-03,  2.80330569e-04,  1.16308499e-03],\n",
              "        [ 4.81237803e-04, -2.58714572e-04,  5.46451251e-04, ...,\n",
              "         -2.28936831e-03, -8.44739334e-05,  1.32249552e-03],\n",
              "        [ 8.32312973e-04, -2.80675857e-04,  4.26743994e-04, ...,\n",
              "         -2.57522380e-03, -4.11504559e-04,  1.48326496e-03]],\n",
              "\n",
              "       [[ 2.01889692e-04,  4.53735774e-05,  1.70246276e-04, ...,\n",
              "         -6.91563546e-05,  6.59728612e-05,  6.05093519e-05],\n",
              "        [ 1.60443029e-04,  2.26515418e-04,  4.89435974e-04, ...,\n",
              "         -1.45351791e-04, -3.59714832e-05,  1.53698384e-05],\n",
              "        [ 2.49691366e-04,  2.91332923e-04,  8.98312894e-04, ...,\n",
              "         -1.00845697e-04, -1.13124761e-05,  1.06110747e-04],\n",
              "        ...,\n",
              "        [ 1.04166356e-04, -2.65741837e-04,  1.54805940e-03, ...,\n",
              "         -8.35671206e-04,  4.26273793e-04, -4.27607389e-04],\n",
              "        [ 3.33304808e-04, -3.81899241e-04,  1.24556501e-03, ...,\n",
              "         -1.25979434e-03,  1.12295602e-04, -2.29472236e-04],\n",
              "        [ 6.43682783e-04, -4.49183717e-04,  9.74046416e-04, ...,\n",
              "         -1.68987596e-03, -1.96736466e-04,  1.04383762e-05]],\n",
              "\n",
              "       [[ 2.01889692e-04,  4.53735774e-05,  1.70246276e-04, ...,\n",
              "         -6.91563546e-05,  6.59728612e-05,  6.05093519e-05],\n",
              "        [ 3.68070585e-04,  1.32565998e-04,  6.89098597e-05, ...,\n",
              "          1.78078655e-04,  2.08441008e-04,  2.88557843e-04],\n",
              "        [ 5.29878249e-04, -1.99110436e-04,  3.25581845e-04, ...,\n",
              "          1.93791508e-04,  3.11873446e-04,  2.75572966e-04],\n",
              "        ...,\n",
              "        [ 1.44883851e-03, -4.79038339e-04,  1.79558338e-04, ...,\n",
              "         -2.32320046e-03, -9.35543678e-04,  4.36706643e-04],\n",
              "        [ 1.78074383e-03, -4.96592838e-04,  1.22465848e-04, ...,\n",
              "         -2.62760720e-03, -1.06475933e-03,  7.46402948e-04],\n",
              "        [ 2.06734659e-03, -4.89151804e-04,  7.27599909e-05, ...,\n",
              "         -2.87585706e-03, -1.14833342e-03,  1.02988433e-03]],\n",
              "\n",
              "       ...,\n",
              "\n",
              "       [[ 2.01889692e-04,  4.53735774e-05,  1.70246276e-04, ...,\n",
              "         -6.91563546e-05,  6.59728612e-05,  6.05093519e-05],\n",
              "        [ 2.80654931e-04,  3.45113396e-04,  7.63436139e-04, ...,\n",
              "         -1.49204076e-04,  2.09484002e-04,  6.87729844e-05],\n",
              "        [ 3.71679460e-04,  3.41692299e-04,  1.14787265e-03, ...,\n",
              "         -8.21709182e-05,  3.18385602e-04,  5.97613107e-05],\n",
              "        ...,\n",
              "        [ 9.14269709e-04, -1.28340523e-03, -1.55484653e-04, ...,\n",
              "         -2.10194499e-03, -5.38292588e-05,  5.71777171e-04],\n",
              "        [ 1.17669185e-03, -1.16779446e-03, -2.23895389e-04, ...,\n",
              "         -2.35396507e-03, -2.67426221e-04,  8.62546731e-04],\n",
              "        [ 1.44664699e-03, -1.03515049e-03, -2.62576912e-04, ...,\n",
              "         -2.56966217e-03, -4.60713840e-04,  1.13045319e-03]],\n",
              "\n",
              "       [[ 2.01889692e-04,  4.53735774e-05,  1.70246276e-04, ...,\n",
              "         -6.91563546e-05,  6.59728612e-05,  6.05093519e-05],\n",
              "        [ 1.76863643e-04,  1.16461590e-04,  1.28756059e-04, ...,\n",
              "         -5.51890589e-05,  2.23512747e-04, -1.62072130e-04],\n",
              "        [ 1.51931963e-04,  2.73971324e-04,  3.69173038e-04, ...,\n",
              "         -9.04278295e-06,  4.44134348e-04, -2.83091067e-04],\n",
              "        ...,\n",
              "        [ 1.83421699e-03, -1.73968147e-04, -4.40155796e-04, ...,\n",
              "         -2.73748790e-03, -8.61355220e-04,  6.27410540e-04],\n",
              "        [ 2.13415269e-03, -1.77428592e-04, -4.88958089e-04, ...,\n",
              "         -3.02126887e-03, -9.84490034e-04,  9.19111189e-04],\n",
              "        [ 2.38569477e-03, -1.70790212e-04, -5.07845951e-04, ...,\n",
              "         -3.24797584e-03, -1.06395688e-03,  1.18711917e-03]],\n",
              "\n",
              "       [[ 2.01889692e-04,  4.53735774e-05,  1.70246276e-04, ...,\n",
              "         -6.91563546e-05,  6.59728612e-05,  6.05093519e-05],\n",
              "        [ 3.68070585e-04,  1.32565998e-04,  6.89098597e-05, ...,\n",
              "          1.78078655e-04,  2.08441008e-04,  2.88557843e-04],\n",
              "        [ 7.04748440e-04,  1.05016379e-05, -1.43037745e-04, ...,\n",
              "          2.58234097e-04,  9.64480496e-05,  3.33273609e-04],\n",
              "        ...,\n",
              "        [ 1.56354532e-03, -1.28415902e-03, -3.44813248e-04, ...,\n",
              "         -1.65492552e-03, -8.05063348e-04,  6.19504892e-04],\n",
              "        [ 1.82549539e-03, -1.10957667e-03, -3.80867452e-04, ...,\n",
              "         -2.00621691e-03, -1.02872809e-03,  8.85172922e-04],\n",
              "        [ 2.08244240e-03, -9.34013922e-04, -4.04733524e-04, ...,\n",
              "         -2.31251889e-03, -1.20566925e-03,  1.13581354e-03]]],\n",
              "      dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bL9tjtChneaK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6ff452e-fbde-4272-b838-696714cbe822"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"text_generator\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        multiple                  3072256   \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  multiple                  5246976   \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                multiple                  8392704   \n",
            "_________________________________________________________________\n",
            "dense (Dense)                multiple                  12301025  \n",
            "=================================================================\n",
            "Total params: 29,012,961\n",
            "Trainable params: 29,012,961\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ttdBelBIwOd6",
        "outputId": "b6843493-ab32-439e-db7e-c233260b35e6"
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True,\n",
        "    reduction='none'\n",
        ")\n",
        "\n",
        "model.compile(loss=loss, optimizer=optimizer)\n",
        "model.fit(dataset, batch_size=256, validation_data=(valset), epochs=10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "488/488 [==============================] - 187s 375ms/step - loss: 3.5099 - val_loss: 3.1696\n",
            "Epoch 2/10\n",
            "488/488 [==============================] - 183s 376ms/step - loss: 3.0269 - val_loss: 2.9665\n",
            "Epoch 3/10\n",
            "488/488 [==============================] - 184s 377ms/step - loss: 2.8583 - val_loss: 2.8538\n",
            "Epoch 4/10\n",
            "488/488 [==============================] - 183s 374ms/step - loss: 2.7336 - val_loss: 2.7725\n",
            "Epoch 5/10\n",
            "488/488 [==============================] - 183s 374ms/step - loss: 2.6291 - val_loss: 2.7087\n",
            "Epoch 6/10\n",
            "488/488 [==============================] - 183s 375ms/step - loss: 2.5344 - val_loss: 2.6600\n",
            "Epoch 7/10\n",
            "488/488 [==============================] - 180s 369ms/step - loss: 2.4468 - val_loss: 2.6119\n",
            "Epoch 8/10\n",
            "488/488 [==============================] - 182s 373ms/step - loss: 2.3644 - val_loss: 2.5730\n",
            "Epoch 9/10\n",
            "488/488 [==============================] - 182s 373ms/step - loss: 2.2864 - val_loss: 2.5424\n",
            "Epoch 10/10\n",
            "488/488 [==============================] - 182s 373ms/step - loss: 2.2125 - val_loss: 2.5138\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fbad37d9950>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B-2DfvZb5uU6",
        "outputId": "2b7e05ee-12ee-4510-80aa-d83d19333df6"
      },
      "source": [
        "embedding_size = 512\n",
        "hidden_size = 2048\n",
        "model = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True,\n",
        "    reduction='none'\n",
        ")\n",
        "\n",
        "model.compile(loss=loss, optimizer=optimizer)\n",
        "model.fit(dataset, batch_size=256, validation_data=(valset), epochs=10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "488/488 [==============================] - 519s 1s/step - loss: 3.2765 - val_loss: 2.9188\n",
            "Epoch 2/10\n",
            "488/488 [==============================] - 503s 1s/step - loss: 2.7419 - val_loss: 2.6691\n",
            "Epoch 3/10\n",
            "488/488 [==============================] - 504s 1s/step - loss: 2.4396 - val_loss: 2.4863\n",
            "Epoch 4/10\n",
            "488/488 [==============================] - 502s 1s/step - loss: 2.1418 - val_loss: 2.3497\n",
            "Epoch 5/10\n",
            "488/488 [==============================] - 501s 1s/step - loss: 1.8573 - val_loss: 2.2502\n",
            "Epoch 6/10\n",
            "488/488 [==============================] - 501s 1s/step - loss: 1.5988 - val_loss: 2.1815\n",
            "Epoch 7/10\n",
            "488/488 [==============================] - 502s 1s/step - loss: 1.3826 - val_loss: 2.1426\n",
            "Epoch 8/10\n",
            "488/488 [==============================] - 501s 1s/step - loss: 1.2142 - val_loss: 2.1343\n",
            "Epoch 9/10\n",
            "488/488 [==============================] - 502s 1s/step - loss: 1.0991 - val_loss: 2.1462\n",
            "Epoch 10/10\n",
            "488/488 [==============================] - 502s 1s/step - loss: 1.0298 - val_loss: 2.1615\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fba663eec50>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ke2GrQGjOXXg"
      },
      "source": [
        "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=20):\n",
        "    # 테스트를 위해서 입력받은 init_sentence도 텐서로 변환합니다\n",
        "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
        "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
        "    end_token = tokenizer.word_index[\"<end>\"]\n",
        "\n",
        "    # 단어 하나씩 예측해 문장을 만듭니다\n",
        "    #    1. 입력받은 문장의 텐서를 입력합니다\n",
        "    #    2. 예측된 값 중 가장 높은 확률인 word index를 뽑아냅니다\n",
        "    #    3. 2에서 예측된 word index를 문장 뒤에 붙입니다\n",
        "    #    4. 모델이 <end>를 예측했거나, max_len에 도달했다면 문장 생성을 마칩니다\n",
        "    while True:\n",
        "        # 1\n",
        "        predict = model(test_tensor) \n",
        "        # 2\n",
        "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1] \n",
        "        # 3 \n",
        "        test_tensor = tf.concat([test_tensor, tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
        "        # 4\n",
        "        if predict_word.numpy()[0] == end_token: break\n",
        "        if test_tensor.shape[1] >= max_len: break\n",
        "\n",
        "    generated = \"\"\n",
        "    # tokenizer를 이용해 word index를 단어로 하나씩 변환합니다 \n",
        "    for word_index in test_tensor[0].numpy():\n",
        "        generated += tokenizer.index_word[word_index] + \" \"\n",
        "\n",
        "    return generated"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4LjvKFXsOZXs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "588d7fef-52d1-44d9-c3cb-3b2205532a9e"
      },
      "source": [
        "generate_text(model, tokenizer, init_sentence=\"<start> i hate\", max_len=20)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'<start> i hate you <end> '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "N5M7wEoCAeOa",
        "outputId": "f2421751-b609-48da-8df9-29db15c2c9e5"
      },
      "source": [
        "generate_text(model, tokenizer, init_sentence=\"<start> i love\", max_len=20)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'<start> i love you , i m not gonna crack <end> '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "WgmlZrwUCcpL",
        "outputId": "ddfb2d06-f257-47e8-eba3-1594caf69d79"
      },
      "source": [
        "generate_text(model, tokenizer, init_sentence=\"<start> i like\", max_len=20)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'<start> i like the way how you re touchin me <end> '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "HDFqFeWrCvLE",
        "outputId": "4396840d-6c43-471d-cb85-8f2aaa7fade2"
      },
      "source": [
        "generate_text(model, tokenizer, init_sentence=\"<start> it is\", max_len=20)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'<start> it is a weeping and a moaning and a gnashing of teeth <end> '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "YqsNXs_7ZfS9",
        "outputId": "0fd21881-ee0d-4af7-e9ed-7b2c36cdd2ef"
      },
      "source": [
        "generate_text(model, tokenizer, init_sentence=\"<start> i\", max_len=20)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'<start> i m not gonna lose any sleep tryna know where you are <end> '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M6o7MV4S2dKq"
      },
      "source": [
        "# 회고 \n",
        "\n",
        "이번 프로젝트를 진행하면서 첫번째로 당황(?)스러웠던 것은 토큰 개수 15개 이하의 문장만 가져오는 것이었다 .\n",
        "\n",
        "```python\n",
        "corpus = []\n",
        "for sentence in raw_corpus:\n",
        "  if len(sentence) == 0: continue\n",
        "  if len(sentence.split(' ')) > 15: continue\n",
        "  corpus.append(preprocess_sentence(sentence))\n",
        "```\n",
        "```python\n",
        "corpus = []\n",
        "for sentence in raw_corpus:\n",
        "  if len(sentence) == 0: continue\n",
        "  corpus.append(preprocess_sentence(sentence))\n",
        "print(len(corpus))\n",
        "corpus[:10]\n",
        "\n",
        "len_corpus = []\n",
        "for sentence in corpus:\n",
        "  if len(sentence.split()) <= 15:\n",
        "    len_corpus.append(sentence)\n",
        "```\n",
        "\n",
        "처음엔 첫 번째 코드로 진행했는데 train.shape이 LMS 기준과 달라 두 번째 코드로 진행하니 기준과 매우 비슷해졌다.\n",
        "\n",
        "처음엔 두 코드가 같은 코드라고 생각했다. 팀원들에게 물어보고 토론한 뒤 첫번째 `corpus`코드는 길이가 15 미만인 문장을 가져와 `preprocess_sentence`를 해주기 때문에 `<start>`와 `<end>`가 붙어 길이가 늘어나는 것을 알게 되었다. \n",
        "\n",
        "두 번째로 당황스러웠던 점은 model을 돌리는데 시간이 너무 많이 소요된다는 점이었다. \n",
        "\n",
        "루브릭 기준을 맞추기 위해 val_loss를 2.2 이하로 낮췄어야 하는데, 첫 번째 시도는 `embedding_size = 256, hidden_size = 1024`으로 하이퍼파라미터를 설정하고, `loss: 2.2125 - val_loss: 2.5138` 정도의 loss를 보였다. \n",
        "\n",
        "두 번째 시도에서는 `embedding_size = 512, hidden_size = 2048`로 하이퍼파라미터 값을 두배로 설정하였더니 `loss: 1.0298 - val_loss: 2.1615` 정도의 loss를 보였다. 분명 loss는 하락했지만, 소요되는 시간이 2배정도 걸렸고 `loss`와 `validation loss`의 차이가 많이 났다. \n",
        "\n",
        "두 번째 모형은 overfitting된 모형으로 해석을 했는데 앞으로 더 배워가면서 하이퍼파라미터를 높여 시간이 많이 소요되는 모형 대신, 전처리 과정을 통해 loss를 줄이는 모형을 만들고 싶다. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XkdfMk6fkihk"
      },
      "source": [
        "[딥러닝을 이용한 자연어 처리 입문](https://wikidocs.net/21698)\n",
        "\n",
        "[Recurrent Networks](https://younnggsuk.github.io/2020/12/23/eecs-12.html)\n",
        "\n",
        "[BackPropagation through LSTM: A differential approach](https://medium.com/@raman.shinde15/backpropagation-through-lstm-a-differential-approach-4eb5ecc58d9d)\n",
        "\n",
        "[딥러닝 RNN - LSTM을 활용한 텍스트 분류 모델](https://blog.naver.com/mini_s0n/222326186376)\n",
        "\n",
        "[RNN과 LSTM을 이해해보자!](https://ratsgo.github.io/natural%20language%20processing/2017/03/09/rnnlstm/)\n",
        "\n",
        "[정규식 : 괄호안에 문자, 문장 제거하기](https://snepbnt.tistory.com/378)"
      ]
    }
  ]
}
